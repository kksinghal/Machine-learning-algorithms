{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "df = sklearn.datasets.load_files(\"./dataset\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print Different Classes \n",
    "\"\"\"\n",
    "\n",
    "classes = df['target_names']\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!saimiri.primate.wisc.edu!tik.vtt.fi!hemuli.tik.vtt.fi!Markku.Savela\\nFrom: Markku.Savela@tel.vtt.fi (Markku Savela)\\nNewsgroups: comp.windows.x\\nSubject: Raster and Text Widgets (View only!), Xew-1.3 version\\nDate: 17 Apr 1993 09:55:18 GMT\\nOrganization: Technical Research Centre of Finland\\nLines: 18\\nDistribution: comp\\nMessage-ID: <1qok66$isa@tik.vtt.fi>\\nReply-To: savela@tel.vtt.fi (Markku Savela)\\nNNTP-Posting-Host: tel4.tel.vtt.fi\\nMime-Version: 1.0\\nContent-Type: text/plain; charset=iso-8859-1\\nContent-Transfer-Encoding: 8bit\\n\\n\\nVersion 1.3 of Xew widgets is available at\\n\\n\\texport.lcs.mit.edu: contrib/Xew-1.3.tar.Z\\n\\texport.lcs.mit.edu: contrib/Xew-1.3.README\\n\\nFor better details, check the README. (For extensive details, you have\\nto with Xew-1.1.ps.Z, still haven't had time to update this one).\\n\\nNo new functionality has been added since 1.2 version. Raster widget\\nhandles now expose events slightly more intelligently than before\\n(really had to do this when I added a simple program that uses X11R5\\nAthena Porthole and Panner widgets). The program demo/viewer.c is\\nvery simple demonstration of panner/porthole usage (copied\\nfrom'editres' actually :-)\\n--\\nMarkku Savela (savela@tel.vtt.fi), Technical Research Centre of Finland\\nTelecommunications Laboratory, Otakaari 7 B, SF-02150 ESPOO, Finland\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample document\n",
    "\"\"\"\n",
    "\n",
    "df.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Preprocessing\n",
    "-------------------\n",
    "Step1 : Remove punctuation and stop words from each document\n",
    "\"\"\"\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "stopWords = set(stopwords.words('english') + list(string.punctuation))\n",
    "for i in range(len(df.data)):\n",
    "    doc = df.data[i] # document at ith index\n",
    "    words = word_tokenize(doc)\n",
    "    df.data[i] = []\n",
    "    for word in words:\n",
    "        if word.lower() not in stopWords:\n",
    "            df.data[i].append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split dataset into train and test set\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df.data, df.target \\\n",
    "                                                    , test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Preprocessing\n",
    "-------------------\n",
    "Step2 : Process train dataset\n",
    "Each row of datset represents unique class and has frequency of unqiue words as features\"\"\"\n",
    "\n",
    "import progressbar\n",
    "bar = progressbar.ProgressBar(maxval=len(Y_train), \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "\n",
    "# Progress bar is used since this process is time consuming\n",
    "X = pd.DataFrame(index = range(len(np.unique(Y_train))))\n",
    "cols_added_since_last_clean = 0\n",
    "for i in range(len(X_train)):\n",
    "    \"\"\"\n",
    "    Remove columns where frequency is too low, to reduce features which do not contribute\n",
    "    to model performance. Do this after every 200 documents\n",
    "    \"\"\"\n",
    "    if(i%200 == 0):\n",
    "        for z in range(-1*cols_added_since_last_clean, -1, 1):\n",
    "            col = X.columns[z]\n",
    "            if(X[col].max() < 10):\n",
    "                X.drop(columns=[col], inplace=True)\n",
    "        cols_added_since_last_clean = 0     \n",
    "        \n",
    "    doc = X_train[i]\n",
    "    rowIndex = Y_train[i]\n",
    "    \n",
    "    \"\"\"\n",
    "    Increment frequency of each word in the document for particular class of document\n",
    "    \"\"\"\n",
    "    for j in range(len(doc)):\n",
    "        word = doc[j]\n",
    "        if(word in X.columns):\n",
    "            X.loc[rowIndex, word] += 1\n",
    "        else:\n",
    "            X[word] = 0\n",
    "            X.loc[rowIndex, word] = 1\n",
    "            cols_added_since_last_clean += 1 \n",
    "    bar.update(i+1)\n",
    "    \n",
    "bar.finish()\n",
    "X_train = X\n",
    "features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Preprocessing\n",
    "-------------------\n",
    "Step3 : Process test dataset\n",
    "Each row of datset represents frequency of unqiue words in each document\n",
    "\"\"\"\n",
    "\n",
    "# Progress bar is used since this process is time consuming\n",
    "bar = progressbar.ProgressBar(maxval=len(X_test), \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "\n",
    "X = pd.DataFrame(columns = features)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    doc = X_test[i]\n",
    "    rowDf = pd.DataFrame(data = [np.zeros(len(X.columns))], columns=X.columns, index=[i])\n",
    "\n",
    "    for j in range(len(doc)):\n",
    "        word = doc[j]\n",
    "        if(word in features):\n",
    "            rowDf.loc[i, word] += 1\n",
    "            \n",
    "    X = X.append(rowDf)\n",
    "    bar.update(i+1)\n",
    "\n",
    "bar.finish()\n",
    "X_test = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use sklearn Multinomial Naive Bayes classifier\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB(alpha=2)\n",
    "classifier.fit(X_train, np.unique(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Sklearn:  0.8005\n"
     ]
    }
   ],
   "source": [
    "accUsingSklearn = classifier.score(X_test, Y_test)\n",
    "print(\"Accuracy using Sklearn: \", accUsingSklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multinomial Naive Bayes\n",
    "------------------------\n",
    "\n",
    "Attributes:\n",
    "    totalCountForEachClass: 1D array\n",
    "        Sum of frequency of each word in each class\n",
    "    totalCount: int\n",
    "        Total count of words in model\n",
    "    model: Naive Bayes Model\n",
    "        Each row of datset represents unique class and has frequency of unqiue words as features\n",
    "    classes: Array\n",
    "        List of classes\n",
    "    alpha: int\n",
    "        Laplace Smoothing factor\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    \"\"\"\n",
    "    Initialiser\n",
    "    ------------\n",
    "    Parameters:\n",
    "    alpha (optional): int\n",
    "        Laplace smoothing factor\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1):\n",
    "        self.totalCountForEachClass = []\n",
    "        self.totalCount = 0\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    \"\"\"\n",
    "    fit() : Create multinomial naive bayes model using training data \n",
    "    -----\n",
    "    Parameters:\n",
    "    X : Pandas Dataframe \n",
    "        Features are frequency of each word in document \n",
    "    Y: array\n",
    "        class labels\n",
    "    \n",
    "    \"\"\"\n",
    "    def fit(self, X, Y):\n",
    "        self.model = X\n",
    "        self.classes = Y\n",
    "        for index, row in X.iterrows():\n",
    "            self.totalCountForEachClass.append(X.loc[index, :].sum())\n",
    "            self.totalCount += self.totalCountForEachClass[index]\n",
    "            \n",
    "            \n",
    "    \"\"\"\n",
    "    predict() : Predict class given features \n",
    "    -----\n",
    "    Parameters:\n",
    "    X : Pandas Dataframe\n",
    "        Each row represent test document\n",
    "        Features are frequency of each word in document \n",
    "        \n",
    "    Returns: array\n",
    "        Labels of predicted class\n",
    "    \"\"\"\n",
    "    def predict(self, X):\n",
    "        logpMax = np.ones(len(X)) # Initialise probability\n",
    "        predClass = -1 * np.ones(len(X)) #Predicted Class\n",
    "        for i in self.classes:\n",
    "            #probability of current class\n",
    "            logp = np.ones(len(X))\n",
    "            for j in range(len(X.columns)):\n",
    "                logp += np.array(X.iloc[:, j] ) * np.log((self.model.iloc[i, j] + self.alpha ) \\\n",
    "                    / ( self.totalCountForEachClass[i] + self.alpha*len(self.model.columns))) \n",
    "            \n",
    "            logp += np.log(self.totalCountForEachClass[i] / self.totalCount)        \n",
    "            for k in range(len(X)):\n",
    "                if logp[k] > logpMax[k] or logpMax[k]==1:\n",
    "                    logpMax[k] = logp[k]\n",
    "                    predClass[k] = i\n",
    "        return predClass\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    score() : Calculate accuracy of model\n",
    "    -----\n",
    "    Parameters:\n",
    "    X : Pandas Dataframe \n",
    "        Features are frequency of each word in document \n",
    "    Y: Array\n",
    "        True Class\n",
    "        \n",
    "    Returns: int\n",
    "        Accuracy of model\n",
    "    \"\"\"\n",
    "    def score(self, X, Y):\n",
    "        pred = self.predict(X)\n",
    "        nCorrectPred = (pred == Y).sum() #Number of correctly classified documents\n",
    "        return nCorrectPred/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNaiveBayes(alpha=2)\n",
    "clf.fit(X_train, np.unique(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.801\n"
     ]
    }
   ],
   "source": [
    "accUsingOwnImpl = clf.score(X_test, Y_test)\n",
    "print(\"Accuracy: \", accUsingOwnImpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Result:\n",
    "\n",
    "Accuracy using Sklearn MultinomialNB = 0.8005\n",
    "Accuracy using my own implementation = 0.801\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
